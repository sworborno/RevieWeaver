{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477fe1a-b158-419e-a5b6-957e4e335e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from uuid import uuid4\n",
    "\n",
    "import pandas as pd\n",
    "from asynciolimiter import Limiter, StrictLimiter\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from pydantic import BaseModel, ValidationError, Field, field_validator\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdbe7c6-0a73-41d4-973d-d4ec1dde2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"YOUR-GCP-PROJECT-NAME\" # We assume you have already authenticated\n",
    "LOCATION = \"us-central1\" # Your GCP project location\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d84c05-92f4-4396-97f8-e954be1b847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_NAME = 'gemini-1.5-flash'\n",
    "llm = VertexAI(\n",
    "    model_name=LLM_NAME,\n",
    "    max_output_tokens=2048,\n",
    "    temperature=0,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3fbcf-8360-4feb-8e1a-a9dfa0c8335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = pd.read_pickle('product_reviews.pkl')\n",
    "review_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab5a13-394a-43e4-8beb-0bbeaecf8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def reformat_data(data: pd.DataFrame):\n",
    "    reviews = []\n",
    "    for id, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        reviews.append(\n",
    "            {\n",
    "                \"PRODUCT_NAME\": row[\"short_name\"],\n",
    "                \"REVIEW_TEXT\": row[\"review_text\"],\n",
    "                \"REVIEW_ID\": row[\"review_id\"],\n",
    "            }\n",
    "        )\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49935977-164b-4651-90c6-5e2337fe7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_reviews = reformat_data(review_data)\n",
    "raw_reviews[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86de00-8a59-4d67-a471-8527ba7ec6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"We have a list of customer reviews for a product. Extract at most 5 features from each REVIEW_TEXT. Features must be relevant to the product attributes or specifications, they must not be representative of a person, or an animal, avoid naive features like (best, product, good). Acronyms should be capitalized according to standard usage (e.g.,  GPS,  USB,  RAM).\n",
    "\n",
    "Here is the review list, formatted as [{\"PRODUCT_NAME\": \"\", \"REVIEW_TEXT\": \"\", \"REVIEW_ID\": \"\"}]:\n",
    "--------------\n",
    "<<REVIEW>>\n",
    "--------------\n",
    "\n",
    "Output the feature indices, feature names with at most two words, the representative sentences in the review, and the associated customer sentiments (Positive or Negative only) in a json object with the following format:\n",
    "\n",
    "**ONLY output the following JSON array.  Do not include any other text.**\n",
    "\n",
    "```json\n",
    "[\n",
    " {\"REVIEW_ID\": \"\", \"ID\": 0, \"FEATURE\": \"\", \"SENTIMENT\": \"Positive\" or \"Negative\", \"REPR_SENTENCE\": \"\"},\n",
    " {\"REVIEW_ID\": \"\", \"ID\": 1, \"FEATURE\": \"\", \"SENTIMENT\": \"Positive\" or \"Negative\", \"REPR_SENTENCE\": \"\"}\n",
    " // ...more objects as needed...\n",
    "]\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a762d-46ec-4b43-86ca-7b55688b031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMOutput(BaseModel):\n",
    "    ReviewId: str = Field(..., alias=\"REVIEW_ID\")\n",
    "    AspectId: int = Field(..., alias=\"ID\")\n",
    "    Aspect: str = Field(..., alias=\"FEATURE\")\n",
    "    Sentiment: str = Field(..., alias=\"SENTIMENT\")\n",
    "    RepresentativeSentence: str = Field(..., alias=\"REPR_SENTENCE\")\n",
    "\n",
    "    model_config = {'populate_by_name': True}\n",
    "\n",
    "    @field_validator('Sentiment') # Changed to @field_validator\n",
    "    def check_sentiment(cls, v):\n",
    "        if v not in [\"Positive\", \"Negative\", \"Neutral\", \"Mixed\"]:\n",
    "            raise ValueError(\"Invalid sentiment value. Must be one of: 'Positive', 'Negative', 'Neutral', 'Mixed'\")\n",
    "        return v\n",
    "    \n",
    "requests_per_minute = 100\n",
    "time_window = 60\n",
    "rate_limiter = StrictLimiter(requests_per_minute/time_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb351fd-715d-4141-9bb1-7ae3f6c77eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def executer(review_objs, progress):\n",
    "    \"\"\"Executes a single LLM request with rate limiting and error handling.\"\"\"\n",
    "    try:\n",
    "        await rate_limiter.wait()\n",
    "        s = time.perf_counter()\n",
    "        result = await llm.ainvoke(\n",
    "            PROMPT.replace(\"<<REVIEW>>\", json.dumps(review_objs))\n",
    "        )\n",
    "        elapsed = time.perf_counter() - s\n",
    "        sleep_time = max(0, time_window - elapsed + 1)  # Ensure sleep_time is non-negative\n",
    "        progress.update()\n",
    "        await asyncio.sleep(sleep_time)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in executer: {e}, Review Objects: {review_objs}\")\n",
    "        return None  # or some other default value indicating failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478da9a9-2f8a-4598-8a8e-993fb5e8ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_llm_executer(review_list, batch_size):\n",
    "    BATCHES = [\n",
    "        review_list[i * batch_size:(i + 1) * batch_size]\n",
    "        for i in range((len(review_list) + batch_size - 1) // batch_size)\n",
    "    ]\n",
    "    pbar = tqdm(total=len(BATCHES), position=0, ncols=90)\n",
    "    pbar.set_description(desc=f\"Requests/min = {requests_per_minute}\", refresh=True)\n",
    "    results = await asyncio.gather(*[executer(review_batch, pbar) for review_batch in BATCHES])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df828f-241d-4292-950f-29807ae8a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_results(raw_aspects):\n",
    "    aspects_flattened = []\n",
    "    # print(f\"Raw aspects input length: {len(rawaspects)}\")\n",
    "    for batch_results in tqdm(raw_aspects, total=len(raw_aspects), desc=\"Processing Batches\"):\n",
    "        # print(f\"Batch results length: {len(batch_results)}\") \n",
    "        if batch_results:\n",
    "            for i in batch_results:\n",
    "                parsed_results = parse_llm_output(i)\n",
    "                aspects_flattened.extend([output.model_dump() for output in parsed_results])\n",
    "\n",
    "    return aspects_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2218db1-2234-4f8e-bbfd-5bac2849d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llm_output(llm_output_str) -> List[LLMOutput]:\n",
    "    \"\"\"Parses the raw string output from the LLM into a list of LLMOutput objects.\"\"\"\n",
    "    import ast\n",
    "    import json\n",
    "\n",
    "    try:\n",
    "        llm_output_str = llm_output_str.replace(\"```\", \"\").replace(\"\\n\", \"\").replace(\"json[\", \"[\")\n",
    "        parsed_output = ast.literal_eval(llm_output_str)\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        try:\n",
    "            llm_output_str = llm_output_str.replace(\"```\", \"\").replace(\"\\n\", \"\").replace(\"json[\", \"[\")\n",
    "            parsed_output = json.loads(llm_output_str)\n",
    "        except json.JSONDecodeError as e2:\n",
    "            print(f\"Error parsing LLM output (JSONDecodeError): {e2} Input String: {llm_output_str}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LLM output (Unexpected Error): {e} Input String: {llm_output_str}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    if isinstance(parsed_output, list):\n",
    "        results = []\n",
    "        for item in parsed_output:\n",
    "            try:\n",
    "                result = LLMOutput(**item)\n",
    "                results.append(result)\n",
    "            except (KeyError, ValueError, TypeError) as e:\n",
    "                print(f\"Error creating LLMOutput object from: {item}. Error: {e}\")\n",
    "        return results\n",
    "    elif isinstance(parsed_output, dict):\n",
    "        try:\n",
    "            return [LLMOutput(**parsed_output)]\n",
    "        except (KeyError, ValueError, TypeError) as e:\n",
    "            print(f\"Error creating LLMOutput object from: {parsed_output}. Error: {e}\")\n",
    "            return []\n",
    "    else:\n",
    "        print(f\"Unexpected LLM output format: {type(parsed_output)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2a8e3-5bc4-4fcd-bed0-d5d0f8d5141a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async_raw_aspects_output = await asyncio.gather(\n",
    "    async_llm_executer(raw_reviews, batch_size=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002a88d-c533-404b-bb68-30f59646eb6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aspects_flattened = post_process_results(async_raw_aspects_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed63176-68e6-4dfd-99e9-bf6d6b560d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aspects_flattened[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c8fad-8265-4d04-9e57-02030a11e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(aspects_flattened)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f964eb0-844d-4114-acbe-5bcb9a2d9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AspectId'] = df['ReviewId'] + \"_\" + df['AspectId'].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e0bf2-3378-481e-9c4e-5eb8da6240ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProductFamilyId'] = df.ReviewId.replace(review_data.set_index('review_id')['product_family_id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff10d7-7c8c-41cd-9822-b07432373b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('product_review_aspects.pkl')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "mypipenv2",
   "name": "pytorch-gpu.2-0.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m109"
  },
  "kernelspec": {
   "display_name": "mypipenv2",
   "language": "python",
   "name": "mypipenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
